{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset 2: \"Students performance in Exams\"</h1>\n",
    "<h2>Task: Data Imputation</h2>\n",
    "Shacha Parker (300235525)\\\n",
    "Callum Frodsham and (300199446)\\\n",
    "Group 79\n",
    "\n",
    "### Setup Instructions To Reproduce this Data Cleaning Notebook:\n",
    "(Step 1 Optional)\n",
    "1. Create a virtual python environment in the project directory (if you want) for all of the packages required:  \n",
    "``` \n",
    "python -m venv .venv\n",
    "```\n",
    "To enter the virutal environment: \n",
    "```\n",
    ".venv/Scripts/activate.ps1 # on windows\n",
    "source .venv/bin/activate # on mac/linux\n",
    "```\n",
    "2. Download all of the required packages (run in cmd/shell of choice):\n",
    "```\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "\n",
    "pip install sklearn\n",
    "#OR\n",
    "pip install scikit-learn\n",
    "```\n",
    "3. VSCode: Ensure you have the correct python kernel selected!\n",
    "<br> \n",
    "If you are using a virtual environment, make sure to select the python interpreter for that virtual environment otherwise this will not work! If you have everything done globally, then just make sure the correct python kernel you are using is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/CLFrod/Assignment2CSI4142/refs/heads/master/StudentsPerformance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "\n",
    "Author: Jakki Seshapanpu\n",
    "\n",
    "\"This data set consists of the marks secured by [US high school] students in various subjects...To understand the influence of the parents background, test preparation etc on students performance\"\n",
    "\n",
    "<h3>Dataset Specifications & Features</h3>\n",
    "\n",
    "The dataset has 8 columns and 1000 rows.\n",
    "\n",
    "Link: <a href=\"https://www.kaggle.com/datasets/spscientist/students-performance-in-exams/data\">Students Performance in Exams</a>\n",
    "\n",
    "### Feature List\n",
    "#### Gender\n",
    "Categorical - Nominal\n",
    "<br>\n",
    "Male/Female\n",
    "#### Race/Ethnicity\n",
    "Categorical - Nominal\n",
    "<br>\n",
    "Referred to as groups ranging from A-E\n",
    "#### Parental Level of Education\n",
    "Categorical - Ordinal\n",
    "<br>\n",
    "Level of education of the student's parents.\n",
    "<br>\n",
    "Arguably, this data type could be nominal. I chose ordinal because there is a discernable difference in level one can make across the diploma types.\n",
    "#### Lunch\n",
    "Categorical - Nominal\n",
    "<br>\n",
    "The type of lunch plan the student is paying for.\n",
    "#### Test Preparation Course\n",
    "Categorical - Nominal\n",
    "<br>\n",
    "If the student had taken the course to prepare for the test or not\n",
    "#### Math Score\n",
    "Numerical - Discrete\n",
    "#### Reading Score\n",
    "Numerical - Discrete\n",
    "#### Writing Score\n",
    "Numerical - Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Test #1\n",
    "We will simulate \"Missing Completely At Random\"-type missing values.\n",
    "<br>\n",
    "For this, the 'gender' column will be used.\n",
    "<br>\n",
    "We can randomly delete entries in the column amounting to a fraction of the total data.\n",
    "<br>\n",
    "Then, we will use Random Sample Imputation to re-fill the empty entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between D1 and D2: 75.6%\n"
     ]
    }
   ],
   "source": [
    "# Make 2 copies of the 'gender' column\n",
    "D1 = data['gender'].copy()\n",
    "D2 = data['gender'].copy()\n",
    "\n",
    "# Calculate the number of entries to drop\n",
    "drop = 0.50 # a simulated percentage\n",
    "num_to_drop = int(len(D2) * drop)\n",
    "\n",
    "# Randomly select entries to drop\n",
    "np.random.seed(0)  # for reproducibility. comment out for multiple tries\n",
    "drop_indices = np.random.choice(D2.index, num_to_drop, replace=False)\n",
    "D2.loc[drop_indices] = np.nan\n",
    "\n",
    "# Fill empty entries in D2 with random samples from D1\n",
    "missing = D2.isnull()\n",
    "num_missing = missing.sum()\n",
    "if num_missing > 0:\n",
    "    sampled_values = D1.dropna().sample(num_missing, replace=True).values\n",
    "    D2.loc[missing] = sampled_values\n",
    "\n",
    "# Compare D1 to D2 in similarity\n",
    "similarity = (D1 == D2).mean()\n",
    "print(f\"Similarity between D1 and D2: {similarity * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Results and Discussion\n",
    "D1-D2 similarity with Random Sample Imputation Accuracy and random seed 0:\n",
    "<br>\n",
    "Format: **drop percentage : D1-D2 similarity percentage : accuracy rating**\n",
    "<br>\n",
    "5% : 96.5% : 30%\n",
    "<br>\n",
    "10% : 95.1% : 51%\n",
    "<br>\n",
    "15% : 93% : 53%\n",
    "<br>\n",
    "20% : 89.9% : 50%\n",
    "<br>\n",
    "33% : 83.8% : 54%\n",
    "<br>\n",
    "50% : 75.6% : 51%\n",
    "<br>\n",
    "For this column, where the types of entries are split close to 50/50, Random Sample Imputation is not very effective in recreating the original entries, with an average accuracy rating of about 50%. Furthermore, I would say that as the types of entries increases past 2, this imputation's effectiveness in finding the correct missing values decreases. Generally, this imputation would be fine if wanting to recreate the proportional amounts of each type of entry, but is ineffective in confidently replicating the original value of the data cell. It is only moderately effective in this case because the chance of error is minimized with there only being Male or Female as entries - which is why I chose it for this experiment. Only accuracy was calculated because precision and recall are trivial for this column's values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Test #2\n",
    "We will simulate \"Missing At Random\"-type missing values.\n",
    "For this, the 'reading score' will be the target for bivariate imputation, with 'writing score' serving as a contextual variable to predict from. In this situation, reading score data is more likely to be missing for those who scored low on the writing score.\n",
    "<br>\n",
    "We'll randomly delete entries in reading score where the writing score is on the lower end (<\\50). Then, we'll use correlational imputation to try to accurately restore the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               math score  reading score  writing score\n",
      "math score       1.000000       0.817580       0.802642\n",
      "reading score    0.817580       1.000000       0.954598\n",
      "writing score    0.802642       0.954598       1.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = data[['math score', 'reading score', 'writing score']].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that reading and writing scores are very highly correlated (0.954598)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_mean = data['reading score'].mean()\n",
    "reading_std = data['reading score'].std()\n",
    "writing_mean = data['writing score'].mean()\n",
    "writing_std = data['writing score'].std()\n",
    "# Make 2 copies of the 'reading score' column\n",
    "D1 = data['reading score'].copy()\n",
    "D2 = data['reading score'].copy()\n",
    "\n",
    "# drop 'drop'% of the entries below the median.\n",
    "drop = 0.1  # 10%\n",
    "below_median_indices = D2[D2 < D2.median()].index\n",
    "num_to_drop = int(len(below_median_indices) * drop * 2)\n",
    "\n",
    "# Randomly select entries to drop from below the median, then turn their value to NaN\n",
    "np.random.seed(0)  # for reproducibility\n",
    "drop_indices = np.random.choice(below_median_indices, num_to_drop, replace=False)\n",
    "D2.loc[drop_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = D2.corr(data['reading score'])\n",
    "\n",
    "def correlationFormula(W):\n",
    "    return(correlation * (reading_std/writing_std) * (W-writing_mean) + reading_mean)\n",
    "\n",
    "# Apply correlation formula to every NaN value in D2\n",
    "for index in D2.index:\n",
    "    # if the value is NaN\n",
    "    if pd.isna(D2.loc[index]):\n",
    "        D2.loc[index] = correlationFormula(data['writing score'].loc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 23.757924386104854 \n",
      "Root Mean Squared Error: 4.8742101294573725 \n",
      "Number of Predictions within +/- 5 : 63 out of 97\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of results\n",
    "MSE = 0\n",
    "closenum = 0\n",
    "closerange = 5\n",
    "for i in drop_indices:\n",
    "    MSE += (D1[i] - D2[i])**2\n",
    "    if -closerange <= (D1[i] - D2[i]) <= closerange:\n",
    "        closenum += 1\n",
    "\n",
    "MSE = MSE / num_to_drop\n",
    "print(\"Mean Squared Error:\", MSE, \"\\nRoot Mean Squared Error:\", MSE**(1/2), \"\\nNumber of Predictions within +/-\", closerange, \":\", closenum, \"out of\", len(drop_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a root mean squared error of ~2 for this column where scores have a valid range of 0-100, I would say this method is a good estimator of the original values in D1.\n",
    "<br>The accuracy ratio test only works if I defined a pre-existing range to determine as \"accurate\". In this case, I defined it as a 'radius' of 5 positive and negative, so a total range of 10. With 63/97 as an accuracy rating, this method is moderately successful in getting a close score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Test #3\n",
    "We will simulate \"Missing Not At Random\"-type missing values.\n",
    "<br>\n",
    "We'll use the numerical values again, this time including math score. Suppose that for some reason, students who have a high math score are permitted to not record their scores, and a certain portion opt into this. This is an unusual case but a valid representation of MNAR. We will then use regression imputation to try to replicate the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = data['math score'].copy()\n",
    "D2 = data['math score'].copy()\n",
    "\n",
    "# First Step - remove a fraction the values classified as \"high grades\"\n",
    "threshold = 80 # changeable if needed\n",
    "high_math_indices = data[data['math score'] > threshold].index\n",
    "\n",
    "drop_percentage = 0.3  # removes a percentage of high math indices\n",
    "num_to_drop = int(len(high_math_indices) * drop_percentage)\n",
    "\n",
    "# Randomly select entries to drop\n",
    "np.random.seed(0)  # for reproducibility\n",
    "drop_indices = np.random.choice(high_math_indices, num_to_drop, replace=False)\n",
    "\n",
    "# Drop the selected entries in the math scores\n",
    "D2.loc[drop_indices] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['reading score', 'writing score']].values\n",
    "y = data['math score'].values\n",
    "\n",
    "# Create model and fit\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Apply model for imputation\n",
    "for index in D2.index:\n",
    "    if pd.isna(D2.loc[index]):\n",
    "        reading_score = data['reading score'].loc[index]\n",
    "        writing_score = data['writing score'].loc[index]\n",
    "        D2.loc[index] = model.predict(np.array([[reading_score, writing_score]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 100.57248346484052 \n",
      "Root Mean Squared Error: 10.028583322924556 \n",
      "Number of Predictions within +/- 5 : 24 out of 52\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of results\n",
    "MSE = 0\n",
    "closenum = 0\n",
    "closerange = 5\n",
    "for i in drop_indices:\n",
    "    MSE += (D1[i] - D2[i])**2\n",
    "    if -closerange <= (D1[i] - D2[i]) <= closerange:\n",
    "        closenum += 1\n",
    "\n",
    "MSE = MSE / num_to_drop\n",
    "print(\"Mean Squared Error:\", MSE, \"\\nRoot Mean Squared Error:\", MSE**(1/2), \"\\nNumber of Predictions within +/-\", closerange, \":\", closenum, \"out of\", len(drop_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression imputation applied to this missing data gave mostly unsatisfactory results. Root mean square error coming up to 10 points and a 40% prediction rate when the prediction range was set to +/-5 suggests that this is not the best type of imputation for this situation. It's likely because regression doesn't work well when the removed data is localized within a dense, small area, rather than being spread across the other entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, we used various methods of imputation to amend simulated 3 missing sata scenarios, each being MCAR, MAR, and MNAR. We removed sections of data according to the situation's conditions, and then applied a chosen method of imputation. We used one univariate, one bivariate, and one multivartiate type of regression: Random Sample Imputation, Correlation Imputation, and Regression Imputation, respectively. After the imputation was performed we evaluated the effectiveness of the imputation using metrics like similarity percentage, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). Random Sample imputation proved to be not very effective in predicting the original values of a binary value in the dataset at an average 50% correct prediction. Correlational imputation performed at a moderately successful rate (about 2/3rds were reasonably close guesses with RMSE of 2). Regressional Imputation did not work well because of its tendency to not predict data well when the area to predict is in a very precise area instead of being spread between other entries. In the future, we could try to test out different imputation methods to see if there would be more optimal results for these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Lecture Slides<br>\n",
    "<a href=\"https://pandas.pydata.org/docs/\">Pandas Documentation</a><br>\n",
    "<a href=\"https://numpy.org/doc/stable/\">NumPy Documentation</a><br>\n",
    "<a href=\"https://scikit-learn.org/stable/user_guide.html\">Scikit-Learn User Guide</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
